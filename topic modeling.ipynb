{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import hdbscan\n",
    "import umap\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "from hdbscan import HDBSCAN\n",
    "\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"./coursera_reviews_processed.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.dropna(subset=[\"processed_reviews\"], inplace=True)\n",
    "data['length'] = data[\"processed_reviews\"].apply(lambda x : len(x.split(\" \")))\n",
    "data = data[data['length'] >= 20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "55540"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "33.79211379186172"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# average length of reviews\n",
    "np.mean(data[\"processed_reviews\"].apply(lambda x : len(x.split(\" \"))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\13064\\anaconda3\\lib\\site-packages\\sklearn\\utils\\deprecation.py:87: FutureWarning: Function get_feature_names is deprecated; get_feature_names is deprecated in 1.0 and will be removed in 1.2. Please use get_feature_names_out instead.\n",
      "  warnings.warn(msg, category=FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "num_features = 200\n",
    "vect = TfidfVectorizer(max_features=num_features)\n",
    "vect.fit(data[\"processed_reviews\"])\n",
    "reviews_dtm = vect.transform(data[\"processed_reviews\"])\n",
    "top_words = vect.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "lda = LatentDirichletAllocation(n_components=4, max_iter=100, random_state=42, n_jobs=-1)\n",
    "lda_prob = lda.fit_transform(reviews_dtm)\n",
    "lda_result = np.argmax(np.array(lda_prob), axis=1)\n",
    "data['lda_result'] = lda_result\n",
    "topic_words_weight = lda.components_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "topic0: [('cours', 2027.3), ('assign', 1400.8), ('data', 1357.2), ('good', 1189.9), ('video', 1153.0), ('would', 1014.6), ('use', 939.2), ('like', 910.2), ('week', 906.5), ('time', 854.2), ('lectur', 833.8), ('question', 828.7), ('materi', 799.3), ('could', 782.3), ('bit', 742.3)] \n",
      "\n",
      "topic1: [('cours', 2127.0), ('thank', 1248.1), ('learn', 1202.7), ('python', 944.8), ('realli', 900.9), ('program', 884.6), ('dr', 881.1), ('great', 811.2), ('languag', 768.5), ('teach', 682.4), ('coursera', 668.1), ('way', 636.2), ('love', 634.8), ('much', 626.9), ('professor', 622.9)] \n",
      "\n",
      "topic2: [('deep', 871.2), ('network', 705.2), ('learn', 699.2), ('andrew', 691.9), ('cours', 657.1), ('step', 624.9), ('ai', 580.5), ('neural', 562.2), ('build', 533.0), ('machin', 507.1), ('project', 435.4), ('understand', 373.4), ('concept', 322.1), ('great', 278.8), ('basic', 250.4)] \n",
      "\n",
      "topic3: [('cours', 2193.7), ('learn', 1396.9), ('thank', 985.9), ('help', 972.1), ('life', 874.0), ('use', 828.7), ('great', 769.9), ('interest', 748.7), ('recommend', 738.0), ('realli', 705.9), ('understand', 692.7), ('manag', 685.8), ('work', 683.8), ('new', 682.0), ('knowledg', 675.1)] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "topic_id = 0\n",
    "for words_weight in topic_words_weight:\n",
    "    word_dict = [(tw, round(ww, 1)) for tw, ww in zip(top_words, words_weight)]\n",
    "    word_dict = sorted(word_dict, key=lambda x: x[1], reverse=True)\n",
    "    word_dict = word_dict[:15]\n",
    "    print('topic%d:' % (topic_id), word_dict, \"\\n\")\n",
    "    topic_id += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "[WinError 1450] 系统资源不足，无法完成请求的服务。",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31m_RemoteTraceback\u001b[0m                          Traceback (most recent call last)",
      "\u001b[1;31m_RemoteTraceback\u001b[0m: \n\"\"\"\nTraceback (most recent call last):\n  File \"C:\\Users\\13064\\anaconda3\\lib\\site-packages\\joblib\\externals\\loky\\process_executor.py\", line 343, in _sendback_result\n    result_queue.put(_ResultItem(work_id, result=result,\n  File \"C:\\Users\\13064\\anaconda3\\lib\\site-packages\\joblib\\externals\\loky\\backend\\queues.py\", line 189, in put\n    self._writer.send_bytes(obj)\n  File \"C:\\Users\\13064\\anaconda3\\lib\\multiprocessing\\connection.py\", line 205, in send_bytes\n    self._send_bytes(m[offset:offset + size])\n  File \"C:\\Users\\13064\\anaconda3\\lib\\multiprocessing\\connection.py\", line 285, in _send_bytes\n    ov, err = _winapi.WriteFile(self._handle, buf, overlapped=True)\nOSError: [WinError 1450] 系统资源不足，无法完成请求的服务。\n\"\"\"",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "Input \u001b[1;32mIn [72]\u001b[0m, in \u001b[0;36m<cell line: 4>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      2\u001b[0m reduce_reviews \u001b[38;5;241m=\u001b[39m u_model\u001b[38;5;241m.\u001b[39mfit_transform(reviews_dtm\u001b[38;5;241m.\u001b[39mtoarray())\n\u001b[0;32m      3\u001b[0m cluster \u001b[38;5;241m=\u001b[39m HDBSCAN(min_cluster_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3\u001b[39m, min_samples\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2000\u001b[39m, metric\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124meuclidean\u001b[39m\u001b[38;5;124m'\u001b[39m, prediction_data\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, gen_min_span_tree\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m----> 4\u001b[0m predict_cluster_result \u001b[38;5;241m=\u001b[39m \u001b[43mcluster\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit_predict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreduce_reviews\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      5\u001b[0m data[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcluster_result\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m predict_cluster_result\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\hdbscan\\hdbscan_.py:1228\u001b[0m, in \u001b[0;36mHDBSCAN.fit_predict\u001b[1;34m(self, X, y)\u001b[0m\n\u001b[0;32m   1213\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfit_predict\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, y\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m   1214\u001b[0m     \u001b[38;5;124;03m\"\"\"Performs clustering on X and returns cluster labels.\u001b[39;00m\n\u001b[0;32m   1215\u001b[0m \n\u001b[0;32m   1216\u001b[0m \u001b[38;5;124;03m    Parameters\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1226\u001b[0m \u001b[38;5;124;03m        cluster labels\u001b[39;00m\n\u001b[0;32m   1227\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 1228\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1229\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlabels_\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\hdbscan\\hdbscan_.py:1190\u001b[0m, in \u001b[0;36mHDBSCAN.fit\u001b[1;34m(self, X, y)\u001b[0m\n\u001b[0;32m   1180\u001b[0m kwargs\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mprediction_data\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[0;32m   1181\u001b[0m kwargs\u001b[38;5;241m.\u001b[39mupdate(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_metric_kwargs)\n\u001b[0;32m   1183\u001b[0m (\n\u001b[0;32m   1184\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlabels_,\n\u001b[0;32m   1185\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprobabilities_,\n\u001b[0;32m   1186\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcluster_persistence_,\n\u001b[0;32m   1187\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_condensed_tree,\n\u001b[0;32m   1188\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_single_linkage_tree,\n\u001b[0;32m   1189\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_min_spanning_tree,\n\u001b[1;32m-> 1190\u001b[0m ) \u001b[38;5;241m=\u001b[39m hdbscan(clean_data, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1192\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmetric \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mprecomputed\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_all_finite:\n\u001b[0;32m   1193\u001b[0m     \u001b[38;5;66;03m# remap indices to align with original data in the case of non-finite entries.\u001b[39;00m\n\u001b[0;32m   1194\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_condensed_tree \u001b[38;5;241m=\u001b[39m remap_condensed_tree(\n\u001b[0;32m   1195\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_condensed_tree, internal_to_raw, outliers\n\u001b[0;32m   1196\u001b[0m     )\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\hdbscan\\hdbscan_.py:822\u001b[0m, in \u001b[0;36mhdbscan\u001b[1;34m(X, min_cluster_size, min_samples, alpha, cluster_selection_epsilon, max_cluster_size, metric, p, leaf_size, algorithm, memory, approx_min_span_tree, gen_min_span_tree, core_dist_n_jobs, cluster_selection_method, allow_single_cluster, match_reference_implementation, **kwargs)\u001b[0m\n\u001b[0;32m    809\u001b[0m         (single_linkage_tree, result_min_span_tree) \u001b[38;5;241m=\u001b[39m memory\u001b[38;5;241m.\u001b[39mcache(\n\u001b[0;32m    810\u001b[0m             _hdbscan_prims_kdtree\n\u001b[0;32m    811\u001b[0m         )(\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    819\u001b[0m             \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[0;32m    820\u001b[0m         )\n\u001b[0;32m    821\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 822\u001b[0m         (single_linkage_tree, result_min_span_tree) \u001b[38;5;241m=\u001b[39m memory\u001b[38;5;241m.\u001b[39mcache(\n\u001b[0;32m    823\u001b[0m             _hdbscan_boruvka_kdtree\n\u001b[0;32m    824\u001b[0m         )(\n\u001b[0;32m    825\u001b[0m             X,\n\u001b[0;32m    826\u001b[0m             min_samples,\n\u001b[0;32m    827\u001b[0m             alpha,\n\u001b[0;32m    828\u001b[0m             metric,\n\u001b[0;32m    829\u001b[0m             p,\n\u001b[0;32m    830\u001b[0m             leaf_size,\n\u001b[0;32m    831\u001b[0m             approx_min_span_tree,\n\u001b[0;32m    832\u001b[0m             gen_min_span_tree,\n\u001b[0;32m    833\u001b[0m             core_dist_n_jobs,\n\u001b[0;32m    834\u001b[0m             \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[0;32m    835\u001b[0m         )\n\u001b[0;32m    836\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:  \u001b[38;5;66;03m# Metric is a valid BallTree metric\u001b[39;00m\n\u001b[0;32m    837\u001b[0m     \u001b[38;5;66;03m# TO DO: Need heuristic to decide when to go to boruvka;\u001b[39;00m\n\u001b[0;32m    838\u001b[0m     \u001b[38;5;66;03m# still debugging for now\u001b[39;00m\n\u001b[0;32m    839\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m X\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m60\u001b[39m:\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\joblib\\memory.py:349\u001b[0m, in \u001b[0;36mNotMemorizedFunc.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    348\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m--> 349\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfunc(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\hdbscan\\hdbscan_.py:325\u001b[0m, in \u001b[0;36m_hdbscan_boruvka_kdtree\u001b[1;34m(X, min_samples, alpha, metric, p, leaf_size, approx_min_span_tree, gen_min_span_tree, core_dist_n_jobs, **kwargs)\u001b[0m\n\u001b[0;32m    322\u001b[0m     X \u001b[38;5;241m=\u001b[39m X\u001b[38;5;241m.\u001b[39mastype(np\u001b[38;5;241m.\u001b[39mfloat64)\n\u001b[0;32m    324\u001b[0m tree \u001b[38;5;241m=\u001b[39m KDTree(X, metric\u001b[38;5;241m=\u001b[39mmetric, leaf_size\u001b[38;5;241m=\u001b[39mleaf_size, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m--> 325\u001b[0m alg \u001b[38;5;241m=\u001b[39m KDTreeBoruvkaAlgorithm(\n\u001b[0;32m    326\u001b[0m     tree,\n\u001b[0;32m    327\u001b[0m     min_samples,\n\u001b[0;32m    328\u001b[0m     metric\u001b[38;5;241m=\u001b[39mmetric,\n\u001b[0;32m    329\u001b[0m     leaf_size\u001b[38;5;241m=\u001b[39mleaf_size \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39m \u001b[38;5;241m3\u001b[39m,\n\u001b[0;32m    330\u001b[0m     approx_min_span_tree\u001b[38;5;241m=\u001b[39mapprox_min_span_tree,\n\u001b[0;32m    331\u001b[0m     n_jobs\u001b[38;5;241m=\u001b[39mcore_dist_n_jobs,\n\u001b[0;32m    332\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[0;32m    333\u001b[0m )\n\u001b[0;32m    334\u001b[0m min_spanning_tree \u001b[38;5;241m=\u001b[39m alg\u001b[38;5;241m.\u001b[39mspanning_tree()\n\u001b[0;32m    335\u001b[0m \u001b[38;5;66;03m# Sort edges of the min_spanning_tree by weight\u001b[39;00m\n",
      "File \u001b[1;32mhdbscan\\_hdbscan_boruvka.pyx:392\u001b[0m, in \u001b[0;36mhdbscan._hdbscan_boruvka.KDTreeBoruvkaAlgorithm.__init__\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mhdbscan\\_hdbscan_boruvka.pyx:426\u001b[0m, in \u001b[0;36mhdbscan._hdbscan_boruvka.KDTreeBoruvkaAlgorithm._compute_bounds\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\joblib\\parallel.py:1056\u001b[0m, in \u001b[0;36m__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m   1053\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_original_iterator \u001b[38;5;241m=\u001b[39m iterator\n\u001b[0;32m   1054\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(pre_dispatch, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mendswith\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[0;32m   1055\u001b[0m     pre_dispatch \u001b[38;5;241m=\u001b[39m eval_expr(\n\u001b[1;32m-> 1056\u001b[0m         pre_dispatch\u001b[38;5;241m.\u001b[39mreplace(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mn_jobs\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28mstr\u001b[39m(n_jobs))\n\u001b[0;32m   1057\u001b[0m     )\n\u001b[0;32m   1058\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pre_dispatch_amount \u001b[38;5;241m=\u001b[39m pre_dispatch \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mint\u001b[39m(pre_dispatch)\n\u001b[0;32m   1060\u001b[0m \u001b[38;5;66;03m# The main thread will consume the first pre_dispatch items and\u001b[39;00m\n\u001b[0;32m   1061\u001b[0m \u001b[38;5;66;03m# the remaining items will later be lazily dispatched by async\u001b[39;00m\n\u001b[0;32m   1062\u001b[0m \u001b[38;5;66;03m# callbacks upon task completions.\u001b[39;00m\n\u001b[0;32m   1063\u001b[0m \n\u001b[0;32m   1064\u001b[0m \u001b[38;5;66;03m# TODO: this iterator should be batch_size * n_jobs\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\joblib\\parallel.py:935\u001b[0m, in \u001b[0;36mretrieve\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    931\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m _verbosity_filter(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_dispatched_batches, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mverbose):\n\u001b[0;32m    932\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[0;32m    933\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_print(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mDone \u001b[39m\u001b[38;5;132;01m%3i\u001b[39;00m\u001b[38;5;124m tasks      | elapsed: \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m    934\u001b[0m                 (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_completed_tasks,\n\u001b[1;32m--> 935\u001b[0m                  short_format_time(elapsed_time), ))\n\u001b[0;32m    936\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    937\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_completed_tasks\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\joblib\\_parallel_backends.py:542\u001b[0m, in \u001b[0;36mwrap_future_result\u001b[1;34m(future, timeout)\u001b[0m\n\u001b[0;32m    539\u001b[0m         warnings\u001b[38;5;241m.\u001b[39mwarn(msg, stacklevel\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3\u001b[39m)\n\u001b[0;32m    541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m--> 542\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39min_main_thread() \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnesting_level \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m):\n\u001b[0;32m    543\u001b[0m     \u001b[38;5;66;03m# Prevent posix fork inside in non-main posix threads\u001b[39;00m\n\u001b[0;32m    544\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m n_jobs \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m    545\u001b[0m         warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[0;32m    546\u001b[0m             \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mLoky-backed parallel loops cannot be nested below \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m    547\u001b[0m             \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mthreads, setting n_jobs=1\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m    548\u001b[0m             stacklevel\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3\u001b[39m)\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\concurrent\\futures\\_base.py:446\u001b[0m, in \u001b[0;36mFuture.result\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m    444\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m CancelledError()\n\u001b[0;32m    445\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_state \u001b[38;5;241m==\u001b[39m FINISHED:\n\u001b[1;32m--> 446\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__get_result\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    447\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    448\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTimeoutError\u001b[39;00m()\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\concurrent\\futures\\_base.py:391\u001b[0m, in \u001b[0;36mFuture.__get_result\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    389\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception:\n\u001b[0;32m    390\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 391\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception\n\u001b[0;32m    392\u001b[0m     \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    393\u001b[0m         \u001b[38;5;66;03m# Break a reference cycle with the exception in self._exception\u001b[39;00m\n\u001b[0;32m    394\u001b[0m         \u001b[38;5;28mself\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[1;31mOSError\u001b[0m: [WinError 1450] 系统资源不足，无法完成请求的服务。"
     ]
    }
   ],
   "source": [
    "u_model = umap.UMAP(n_components=5)\n",
    "reduce_reviews = u_model.fit_transform(reviews_dtm.toarray())\n",
    "cluster = HDBSCAN(min_cluster_size=3, min_samples=2000, metric='euclidean', prediction_data=True, gen_min_span_tree=True)\n",
    "predict_cluster_result = cluster.fit_predict(reduce_reviews)\n",
    "data[\"cluster_result\"] = predict_cluster_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "-1\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "label_num_dict = dict(Counter(cluster.labels_))\n",
    "for k in label_num_dict:\n",
    "    if label_num_dict[k] > 1000:\n",
    "        print(k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2         recommend wide cours reach import inform weste...\n",
       "5         far best cours html css javascript great instr...\n",
       "8         cours brilliant succinct scientif illustr prov...\n",
       "15        part predict analyt case studi actual good two...\n",
       "18        think cours everyon industrialis societi eat e...\n",
       "                                ...                        \n",
       "289814    cours realli explain core principl ux deal rea...\n",
       "289817    final exam question wrong none select answer c...\n",
       "289819    great content mathemat idea well explain probl...\n",
       "289822    realli like cours teacher explain well learnt ...\n",
       "289824    good cours overal last two week assign littl b...\n",
       "Name: processed_reviews, Length: 45287, dtype: object"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[data['cluster_result'] == 0]['processed_reviews']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.to_csv(\"topic_result.csv\", encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
